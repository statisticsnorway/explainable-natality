{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "deletable": true,
    "editable": true,
    "id": "qnMpW5Y9nv2l"
   },
   "outputs": [],
   "source": [
    "# Copyright 2020 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\n",
    "# NOTICE OF MAJOR CHANGES: \n",
    "# In SEPTEMBER 2021 this code has been altered by Olve Drageset as part of a Hackathon hosted by Statistics Norway.\n",
    "# Firstly, the code is altered to run on the current generation of Google AI Platform, using Tensorflow 2.5 (instead of 2.1), and the 2.5 Tensorflow Runtime of the GCP AI Platform (instead of 2.1). \n",
    "# Secondly, the code has also been altered to load Natality data that has been extracted from an open BigQuery dataset to CSV by another notebook.\n",
    "# Thirdly, the Deep Neural Network model itself and its training procedure has been altered to accomodate the Natality data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "mHF9VCProKJN"
   },
   "source": [
    "# AI Explanations: Explaining a tabular data model\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/ai-platform-samples/tree/master/notebooks/samples/explanations/tf2/ai-explanations-tabular.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/ai-platform-samples/tree/master/notebooks/samples/explanations/tf2/ai-explanations-tabular.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "hZzRVxNtH-zG"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This tutorial shows how to train a Keras model on tabular data and deploy it to the AI Explanations service to get feature attributions on your deployed model.\n",
    "\n",
    "If you've already got a trained model and want to deploy it to AI Explanations, skip to the **Export the model as a TF 2 SavedModel** section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "iN69d4D9Flrh"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset used for this tutorial was created by combining two BigQuery Public Datasets: [London Bikeshare data](https://console.cloud.google.com/marketplace/details/greater-london-authority/london-bicycles?filter=solution-type%3Adataset&q=london%20bicycle%20hires&id=95374cac-2834-4fa2-a71f-fc033ccb5ce4) and [NOAA weather data](https://console.cloud.google.com/marketplace/details/noaa-public/gsod?filter=solution-type:dataset&q=noaa&id=c6c1b652-3958-4a47-9e58-552a546df47f). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "Su2qu-4CW-YH"
   },
   "source": [
    "### Objective\n",
    "\n",
    "The goal is to train a model using the Keras Sequential API that predicts how long a bike trip took based on the trip start time, distance, day of week, and various weather data during that day. \n",
    "\n",
    "This tutorial focuses more on deploying the model to AI Explanations than on the design of the model itself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "912RD_3fxGeH"
   },
   "source": [
    "### Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud Platform (GCP):\n",
    "\n",
    "* AI Platform for:\n",
    "  * Prediction\n",
    "  * Explanation: AI Explanations comes at no extra charge to prediction prices. However, explanation requests take longer to process than normal predictions, so heavy usage of AI Explanations along with auto-scaling may result in more nodes being started and thus more charges\n",
    "* Cloud Storage for:\n",
    "  * Storing model files for deploying to Cloud AI Platform\n",
    "\n",
    "Learn about [AI Platform\n",
    "pricing](https://cloud.google.com/ml-engine/docs/pricing) and [Cloud Storage\n",
    "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "rgLXkyHEvTVD"
   },
   "source": [
    "## Before you begin\n",
    "\n",
    "Make sure you're running this notebook in a **GPU runtime** if you have that option. In Colab, select **Runtime** --> **Change runtime type**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "avDUUQEGTnUo"
   },
   "source": [
    "This tutorial assumes you are running the notebook either in **Colab** or **Cloud AI Platform Notebooks**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "i2qsxysTVc-l"
   },
   "source": [
    "### Set up your GCP project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a GCP project.](https://console.cloud.google.com/cloud-resource-manager)\n",
    "\n",
    "2. [Make sure that billing is enabled for your project.](https://cloud.google.com/billing/docs/how-to/modify-project)\n",
    "\n",
    "3. [Enable the AI Platform Training & Prediction and Compute Engine APIs.](https://console.cloud.google.com/flows/enableapi?apiid=ml.googleapis.com,compute_component)\n",
    "\n",
    "4. Enter your project ID in the cell below. Then run the  cell to make sure the\n",
    "Cloud SDK uses the right project for all the commands in this notebook.\n",
    "\n",
    "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8fBAiI4z9iV2"
   },
   "source": [
    "#### Project ID\n",
    "\n",
    "**If you don't know your project ID.**\n",
    "\n",
    "You might able to get your project ID using `gcloud` command, by executing the second code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "deletable": true,
    "editable": true,
    "id": "4qxwBA4RM9Lu"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"hack4ssb-team1\" #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PirKTpvc9iV8"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RisfuQYn9lJJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "!gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RRmsf7jG9iV_"
   },
   "source": [
    "#### Region\n",
    "\n",
    "You can also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook. Make sure to [choose a region where Cloud\n",
    "AI Platform services are\n",
    "available](https://cloud.google.com/ml-engine/docs/tensorflow/regions). You can\n",
    "not use a Multi-Regional Storage bucket for training with AI Platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MqKd1EUc9iWA"
   },
   "outputs": [],
   "source": [
    "REGION = 'europe-west3' #@param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "chZ3Xxlu9iWC"
   },
   "source": [
    "#### Timestamp\n",
    "\n",
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, we create a timestamp for each instance session, and append onto the name of resources which will be created in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VxDuCyvx9iWD"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "TSy-f05IO4LB"
   },
   "source": [
    "### Authenticate your GCP account\n",
    "\n",
    "**If you are using AI Platform Notebooks**, your environment is already\n",
    "authenticated. Skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "fZQUrHdXNJnk"
   },
   "source": [
    "**If you are using Colab**, run the cell below and follow the instructions\n",
    "when prompted to authenticate your account via oAuth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": true,
    "editable": true,
    "id": "W9i6oektpgld"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tabulate in /opt/conda/lib/python3.7/site-packages (0.8.9)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "# If you are running this notebook in Colab, follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "\n",
    "def install_dlvm_packages():\n",
    "    ! pip install tabulate\n",
    "\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import auth as google_auth\n",
    "    google_auth.authenticate_user()\n",
    "    ! pip install witwidget --quiet\n",
    "    ! pip install tensorflow==2.5 --quiet\n",
    "    ! gcloud config set project $PROJECT_ID\n",
    "\n",
    "elif \"DL_PATH\" in os.environ:\n",
    "    install_dlvm_packages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "tT061irlJwkg"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "When you submit a training job using the Cloud SDK, you upload a Python package\n",
    "containing your training code to a Cloud Storage bucket. AI Platform runs\n",
    "the code from this package. In this tutorial, AI Platform also saves the\n",
    "trained model that results from your job in the same bucket. You can then\n",
    "create an AI Platform model version based on this output in order to serve\n",
    "online predictions.\n",
    "\n",
    "Set the name of your Cloud Storage bucket below. It must be unique across all\n",
    "Cloud Storage buckets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6uy_VUPw9iWL"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"dev-xai\" #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": true,
    "editable": true,
    "id": "bTxmbDg1I0x1"
   },
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"[your-bucket-name]\":\n",
    "    BUCKET_NAME = PROJECT_ID + \"_xai_flowers_\" + TIMESTAMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "fsmCk2dwJnLZ"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": true,
    "editable": true,
    "id": "160PRO3aJqLD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://dev-xai/...\n",
      "ServiceException: 409 A Cloud Storage bucket named 'dev-xai' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"
     ]
    }
   ],
   "source": [
    "! gsutil mb -l $REGION gs://$BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PyxoF-iqqD1t"
   },
   "source": [
    "### Import libraries\n",
    "\n",
    "Import the libraries for this tutorial. This tutorial has been tested with **TensorFlow versions 2.1 and 2.2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MEDlLSWK15UL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.5.0\n",
      "Requirement already satisfied: explainable-ai-sdk in /opt/conda/lib/python3.7/site-packages (1.3.2)\n",
      "Requirement already satisfied: google.auth>=1.14.1 in /opt/conda/lib/python3.7/site-packages (from explainable-ai-sdk) (1.35.0)\n",
      "Requirement already satisfied: ipython in /opt/conda/lib/python3.7/site-packages (from explainable-ai-sdk) (7.28.0)\n",
      "Requirement already satisfied: requests>=2.5 in /opt/conda/lib/python3.7/site-packages (from explainable-ai-sdk) (2.26.0)\n",
      "Requirement already satisfied: numpy>=1.7 in /opt/conda/lib/python3.7/site-packages (from explainable-ai-sdk) (1.19.5)\n",
      "Requirement already satisfied: xai-image-widget in /opt/conda/lib/python3.7/site-packages (from explainable-ai-sdk) (0.1.0)\n",
      "Requirement already satisfied: xai-tabular-widget in /opt/conda/lib/python3.7/site-packages (from explainable-ai-sdk) (0.1.0)\n",
      "Requirement already satisfied: tensorflow>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from explainable-ai-sdk) (2.5.0)\n",
      "Requirement already satisfied: matplotlib>=3.2.2 in /opt/conda/lib/python3.7/site-packages (from explainable-ai-sdk) (3.4.3)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google.auth>=1.14.1->explainable-ai-sdk) (58.0.4)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from google.auth>=1.14.1->explainable-ai-sdk) (1.15.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google.auth>=1.14.1->explainable-ai-sdk) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google.auth>=1.14.1->explainable-ai-sdk) (0.2.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google.auth>=1.14.1->explainable-ai-sdk) (4.2.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.2.2->explainable-ai-sdk) (8.3.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.2.2->explainable-ai-sdk) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.2.2->explainable-ai-sdk) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.2.2->explainable-ai-sdk) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.2.2->explainable-ai-sdk) (0.10.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google.auth>=1.14.1->explainable-ai-sdk) (0.4.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.5->explainable-ai-sdk) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.5->explainable-ai-sdk) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.5->explainable-ai-sdk) (2021.5.30)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests>=2.5->explainable-ai-sdk) (2.0.0)\n",
      "Requirement already satisfied: tensorboard~=2.5 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.15.0->explainable-ai-sdk) (2.6.0)\n",
      "Requirement already satisfied: gast==0.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.15.0->explainable-ai-sdk) (0.4.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.15.0->explainable-ai-sdk) (1.1.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.15.0->explainable-ai-sdk) (1.6.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.15.0->explainable-ai-sdk) (2.5.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.15.0->explainable-ai-sdk) (1.12)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.15.0->explainable-ai-sdk) (3.18.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.15.0->explainable-ai-sdk) (0.37.0)\n",
      "Requirement already satisfied: keras-nightly~=2.5.0.dev in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.15.0->explainable-ai-sdk) (2.5.0.dev2021032900)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.15.0->explainable-ai-sdk) (1.12.1)\n",
      "Requirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.15.0->explainable-ai-sdk) (0.14.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.15.0->explainable-ai-sdk) (3.7.4.3)\n",
      "Requirement already satisfied: grpcio~=1.34.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.15.0->explainable-ai-sdk) (1.34.1)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.15.0->explainable-ai-sdk) (0.2.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.15.0->explainable-ai-sdk) (1.1.2)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.15.0->explainable-ai-sdk) (3.3.0)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=1.15.0->explainable-ai-sdk) (3.1.0)\n",
      "Requirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py~=3.1.0->tensorflow>=1.15.0->explainable-ai-sdk) (1.5.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow>=1.15.0->explainable-ai-sdk) (3.3.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow>=1.15.0->explainable-ai-sdk) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow>=1.15.0->explainable-ai-sdk) (1.8.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow>=1.15.0->explainable-ai-sdk) (2.0.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow>=1.15.0->explainable-ai-sdk) (0.6.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=1.15.0->explainable-ai-sdk) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow>=1.15.0->explainable-ai-sdk) (4.8.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=1.15.0->explainable-ai-sdk) (3.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.5->tensorflow>=1.15.0->explainable-ai-sdk) (3.5.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.7/site-packages (from ipython->explainable-ai-sdk) (4.8.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in /opt/conda/lib/python3.7/site-packages (from ipython->explainable-ai-sdk) (5.1.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.7/site-packages (from ipython->explainable-ai-sdk) (0.7.5)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.7/site-packages (from ipython->explainable-ai-sdk) (0.18.0)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.7/site-packages (from ipython->explainable-ai-sdk) (0.2.0)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.7/site-packages (from ipython->explainable-ai-sdk) (0.1.3)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from ipython->explainable-ai-sdk) (4.4.2)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from ipython->explainable-ai-sdk) (3.0.20)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.7/site-packages (from ipython->explainable-ai-sdk) (2.10.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from jedi>=0.16->ipython->explainable-ai-sdk) (0.8.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.7/site-packages (from pexpect>4.3->ipython->explainable-ai-sdk) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->explainable-ai-sdk) (0.2.5)\n",
      "Requirement already satisfied: ipywidgets>=7.0.0 in /opt/conda/lib/python3.7/site-packages (from xai-image-widget->explainable-ai-sdk) (7.6.5)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.7/site-packages (from ipywidgets>=7.0.0->xai-image-widget->explainable-ai-sdk) (6.4.1)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets>=7.0.0->xai-image-widget->explainable-ai-sdk) (5.1.3)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets>=7.0.0->xai-image-widget->explainable-ai-sdk) (0.2.0)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets>=7.0.0->xai-image-widget->explainable-ai-sdk) (1.0.2)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets>=7.0.0->xai-image-widget->explainable-ai-sdk) (3.5.1)\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->xai-image-widget->explainable-ai-sdk) (6.1)\n",
      "Requirement already satisfied: jupyter-client<8.0 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->xai-image-widget->explainable-ai-sdk) (7.0.4)\n",
      "Requirement already satisfied: argcomplete>=1.12.3 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->xai-image-widget->explainable-ai-sdk) (1.12.3)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->xai-image-widget->explainable-ai-sdk) (1.4.1)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in /opt/conda/lib/python3.7/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets>=7.0.0->xai-image-widget->explainable-ai-sdk) (1.5.1)\n",
      "Requirement already satisfied: pyzmq>=13 in /opt/conda/lib/python3.7/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets>=7.0.0->xai-image-widget->explainable-ai-sdk) (22.3.0)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.7/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets>=7.0.0->xai-image-widget->explainable-ai-sdk) (0.3)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in /opt/conda/lib/python3.7/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets>=7.0.0->xai-image-widget->explainable-ai-sdk) (4.8.1)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->xai-image-widget->explainable-ai-sdk) (3.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>=7.0.0->xai-image-widget->explainable-ai-sdk) (0.17.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>=7.0.0->xai-image-widget->explainable-ai-sdk) (21.2.0)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /opt/conda/lib/python3.7/site-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->xai-image-widget->explainable-ai-sdk) (6.4.4)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->xai-image-widget->explainable-ai-sdk) (2.11.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->xai-image-widget->explainable-ai-sdk) (0.12.1)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->xai-image-widget->explainable-ai-sdk) (1.8.0)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->xai-image-widget->explainable-ai-sdk) (20.1.0)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->xai-image-widget->explainable-ai-sdk) (0.11.0)\n",
      "Requirement already satisfied: nbconvert in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->xai-image-widget->explainable-ai-sdk) (6.2.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->xai-image-widget->explainable-ai-sdk) (1.14.6)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->xai-image-widget->explainable-ai-sdk) (2.20)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.7/site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->xai-image-widget->explainable-ai-sdk) (1.1.1)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->xai-image-widget->explainable-ai-sdk) (0.7.1)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->xai-image-widget->explainable-ai-sdk) (4.1.0)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->xai-image-widget->explainable-ai-sdk) (0.5.4)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->xai-image-widget->explainable-ai-sdk) (0.8.4)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->xai-image-widget->explainable-ai-sdk) (0.5.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->xai-image-widget->explainable-ai-sdk) (1.5.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->xai-image-widget->explainable-ai-sdk) (0.1.2)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.7/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->xai-image-widget->explainable-ai-sdk) (0.5.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->xai-image-widget->explainable-ai-sdk) (21.0)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "# should be >= 2.5\n",
    "print(\"Tensorflow version \" + tf.__version__)\n",
    "if tf.__version__ < \"2.5\":\n",
    "    raise Exception(\"TF 2.5 or greater is required\")\n",
    "\n",
    "!pip install explainable-ai-sdk\n",
    "import explainable_ai_sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "aRVMEU2Qshm4"
   },
   "source": [
    "## Download and preprocess the data\n",
    "\n",
    "In this section you'll download the data to train your model from a public GCS bucket. The original data is from the BigQuery datasets linked above. For your convenience, we've joined the London bike and NOAA weather tables, done some preprocessing, and provided a subset of that dataset here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v7HLNsvekxvz"
   },
   "outputs": [],
   "source": [
    "# GET DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "8zr6lj66UlMn"
   },
   "source": [
    "### Read the data with Pandas\n",
    "\n",
    "You'll use Pandas to read the data into a `DataFrame` and then do some additional pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": true,
    "editable": true,
    "id": "Icz22E69smnD"
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.csv', names=['weight_pounds', 'is_male', 'mother_age', 'plurality', 'gestation_weeks', 'key'])\n",
    "eval_data = pd.read_csv('eval.csv', names=['weight_pounds', 'is_male', 'mother_age', 'plurality', 'gestation_weeks', 'key'])\n",
    "data = pd.concat([train_data, eval_data])\n",
    "\n",
    "# Shuffle the data\n",
    "data = data.sample(frac=1, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FAuenzty9iWf"
   },
   "source": [
    "Let's take a look at the first five rows of your data in the panda dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vxZryg4xmdy0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of data: 16336\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight_pounds</th>\n",
       "      <th>is_male</th>\n",
       "      <th>mother_age</th>\n",
       "      <th>plurality</th>\n",
       "      <th>gestation_weeks</th>\n",
       "      <th>key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5237</th>\n",
       "      <td>8.157104</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>40.0</td>\n",
       "      <td>8599690069971956834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2471</th>\n",
       "      <td>7.198093</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>39.0</td>\n",
       "      <td>-1866590652208008467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3759</th>\n",
       "      <td>9.186662</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>41.0</td>\n",
       "      <td>7586701093880244808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3414</th>\n",
       "      <td>7.625790</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>3042228741091961920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>7.874912</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>40.0</td>\n",
       "      <td>5934265245228309013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      weight_pounds  is_male  mother_age  plurality  gestation_weeks  \\\n",
       "5237       8.157104        0          30          1             40.0   \n",
       "2471       7.198093        1          18          1             39.0   \n",
       "3759       9.186662        0          21          1             41.0   \n",
       "3414       7.625790        1          28          1             35.0   \n",
       "1065       7.874912        1          30          1             40.0   \n",
       "\n",
       "                      key  \n",
       "5237  8599690069971956834  \n",
       "2471 -1866590652208008467  \n",
       "3759  7586701093880244808  \n",
       "3414  3042228741091961920  \n",
       "1065  5934265245228309013  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview the first 5 rows\n",
    "print(f\"length of data: {len(data)}\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rXfCbcox9iWi"
   },
   "source": [
    "Next, you will separate the data into features ('data') and labels ('labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YXLNDcfUvlr8"
   },
   "outputs": [],
   "source": [
    "# Save duration to its own DataFrame and remove it from the original DataFrame\n",
    "labels = data['weight_pounds']\n",
    "data = data.drop(columns=['weight_pounds', 'key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "iSrzwuchvcgv"
   },
   "source": [
    "### Split data into train and test sets\n",
    "\n",
    "You'll split your data into train and test sets using an 80 / 20 train / test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": true,
    "editable": true,
    "id": "D5PIljnYveDN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 13068\n",
      "Test size: 3268\n"
     ]
    }
   ],
   "source": [
    "# Use 80/20 train/test split\n",
    "train_size = int(len(data) * .8)\n",
    "\n",
    "# Split your data into train and test sets\n",
    "train_data = data[:train_size]\n",
    "train_labels = labels[:train_size]\n",
    "print(\"Train size: %d\" % len(train_data))\n",
    "\n",
    "test_data = data[train_size:]\n",
    "test_labels = labels[train_size:]\n",
    "print(\"Test size: %d\" % len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kV_NEAQwwH0e"
   },
   "source": [
    "## Build, train, and evaluate your model with Keras\n",
    "\n",
    "This section shows how to build, train, evaluate, and get local predictions from a model by using the Keras [Sequential API](https://www.tensorflow.org/guide/keras/sequential_model). The model will takes your 10 features as input and predict the trip duration in minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3kQz8Q0DsBM7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input dimensions: 4\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import initializers\n",
    "input_dimensions=len(train_data.iloc[0])\n",
    "print(f\"input dimensions: {input_dimensions}\")\n",
    "# Build your model\n",
    "model = tf.keras.Sequential(name=\"babyweight_predict\")\n",
    "model.add(tf.keras.layers.Dense(64, \n",
    "                                input_dim=len(train_data.iloc[0]), \n",
    "                                activation='sigmoid', \n",
    "                                kernel_initializer=initializers.RandomNormal(stddev=0.01),\n",
    "                                bias_initializer=initializers.Zeros()))\n",
    "model.add(tf.keras.layers.Dense(32, \n",
    "                                activation='sigmoid', \n",
    "                                kernel_initializer=initializers.RandomNormal(stddev=0.0Â¢1),\n",
    "                                bias_initializer=initializers.Zeros()))\n",
    "model.add(tf.keras.layers.Dense(1, activation=\"linear\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UvAcjSUcs_l7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"babyweight_predict\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                320       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,433\n",
      "Trainable params: 2,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compile the model and see a summary\n",
    "optimizer = tf.keras.optimizers.Adam(0.01)\n",
    "model.compile(loss='mean_squared_logarithmic_error', optimizer=optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GcOkuHPVwjiM"
   },
   "source": [
    "### Create an input data pipeline with tf.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZUu9wFklwmm6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13068\n",
      "204\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "input_train = tf.data.Dataset.from_tensor_slices(train_data)\n",
    "print(len(input_train))\n",
    "output_train = tf.data.Dataset.from_tensor_slices(train_labels)\n",
    "input_train = input_train.batch(batch_size, drop_remainder=True)\n",
    "nr_of_batches = print(len(input_train))\n",
    "input_train = input_train.repeat()\n",
    "output_train = output_train.batch(batch_size, drop_remainder=True).repeat()\n",
    "train_dataset = tf.data.Dataset.zip((input_train, output_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l98aRzfPwo5e"
   },
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h1x_8CR0wtRs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "204/204 [==============================] - 1s 1ms/step - loss: 0.0682\n",
      "Epoch 2/10\n",
      "204/204 [==============================] - 0s 1ms/step - loss: 0.0381\n",
      "Epoch 3/10\n",
      "204/204 [==============================] - 0s 1ms/step - loss: 0.0380\n",
      "Epoch 4/10\n",
      "204/204 [==============================] - 0s 1ms/step - loss: 0.0357\n",
      "Epoch 5/10\n",
      "204/204 [==============================] - 0s 1ms/step - loss: 0.0288\n",
      "Epoch 6/10\n",
      "204/204 [==============================] - 0s 1ms/step - loss: 0.0280\n",
      "Epoch 7/10\n",
      "204/204 [==============================] - 0s 1ms/step - loss: 0.0264\n",
      "Epoch 8/10\n",
      "204/204 [==============================] - 0s 1ms/step - loss: 0.0257\n",
      "Epoch 9/10\n",
      "204/204 [==============================] - 0s 1ms/step - loss: 0.0236\n",
      "Epoch 10/10\n",
      "204/204 [==============================] - 0s 1ms/step - loss: 0.0228\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3b5ec1cb10>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will take about a minute to run\n",
    "# To keep training time short, you're not using the full dataset\n",
    "model.fit(train_dataset, steps_per_epoch=train_size // batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QPr0A8bjw0wm"
   },
   "source": [
    "### Evaluate the trained model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Elbvna4vU30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 0s 835us/step - loss: 0.0235\n",
      "0.023524289950728416\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation\n",
    "results = model.evaluate(test_data, test_labels)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bIh6uds2x2tr"
   },
   "outputs": [],
   "source": [
    "# Send test instances to model for prediction\n",
    "predict = model.predict(test_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aFjBh4DVx7QL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted weight: 7\n",
      "Actual weight: 9.25059651352 \n",
      "\n",
      "Predicted weight: 7\n",
      "Actual weight: 6.75055446244 \n",
      "\n",
      "Predicted weight: 7\n",
      "Actual weight: 7.495716907999999 \n",
      "\n",
      "Predicted weight: 7\n",
      "Actual weight: 8.375361333379999 \n",
      "\n",
      "Predicted weight: 7\n",
      "Actual weight: 5.37486994756 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preview predictions on the first 5 examples from your test dataset\n",
    "for i, val in enumerate(predict):\n",
    "    print('Predicted weight: {}'.format(round(val[0])))\n",
    "    print('Actual weight: {} \\n'.format(test_labels.iloc[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "gAO6-zv6osJ8"
   },
   "source": [
    "## Export the model as a TF 2.x SavedModel\n",
    "\n",
    "When using TensorFlow 2.x, you export the model as a `SavedModel` and load it into Cloud Storage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uV6l3_qF9iW7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gs://dev-xai/explanations/mymodel/assets\n",
      "gs://dev-xai/explanations/mymodel\n"
     ]
    }
   ],
   "source": [
    "export_path = 'gs://' + BUCKET_NAME + '/explanations/mymodel'\n",
    "model.save(export_path)\n",
    "print(export_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-f8elyM8KMNX"
   },
   "source": [
    "Use TensorFlow's `saved_model_cli` to inspect the model's SignatureDef. You'll use this information when you deploy your model to AI Explanations in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yFg5r-7s1BKr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
      "\n",
      "signature_def['__saved_model_init_op']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['__saved_model_init_op'] tensor_info:\n",
      "        dtype: DT_INVALID\n",
      "        shape: unknown_rank\n",
      "        name: NoOp\n",
      "  Method name is: \n",
      "\n",
      "signature_def['serving_default']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['dense_input'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 4)\n",
      "        name: serving_default_dense_input:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['dense_2'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 1)\n",
      "        name: StatefulPartitionedCall:0\n",
      "  Method name is: tensorflow/serving/predict\n",
      "\n",
      "Defined Functions:\n",
      "  Function Name: '__call__'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          inputs: TensorSpec(shape=(None, 4), dtype=tf.float32, name='inputs')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #2\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          dense_input: TensorSpec(shape=(None, 4), dtype=tf.float32, name='dense_input')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #3\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          inputs: TensorSpec(shape=(None, 4), dtype=tf.float32, name='inputs')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #4\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          dense_input: TensorSpec(shape=(None, 4), dtype=tf.float32, name='dense_input')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "\n",
      "  Function Name: '_default_save_signature'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          dense_input: TensorSpec(shape=(None, 4), dtype=tf.float32, name='dense_input')\n",
      "\n",
      "  Function Name: 'call_and_return_all_conditional_losses'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          inputs: TensorSpec(shape=(None, 4), dtype=tf.float32, name='inputs')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #2\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          dense_input: TensorSpec(shape=(None, 4), dtype=tf.float32, name='dense_input')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #3\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          dense_input: TensorSpec(shape=(None, 4), dtype=tf.float32, name='dense_input')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #4\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          inputs: TensorSpec(shape=(None, 4), dtype=tf.float32, name='inputs')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n"
     ]
    }
   ],
   "source": [
    "! saved_model_cli show --dir $export_path --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y270ZNinycoy"
   },
   "source": [
    "## Deploy the model to AI Explanations\n",
    "\n",
    "In order to deploy the model to Explanations, you need to generate an `explanations_metadata.json` file and upload this to the Cloud Storage bucket with your SavedModel. Then you'll deploy the model using `gcloud`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cUdUVjjGbvQy"
   },
   "source": [
    "### Prepare explanation metadata\n",
    "\n",
    "In order to deploy this model to AI Explanations, you need to create an explanation_metadata.json file with information about your model inputs, outputs, and baseline. You can use the [Explainable AI SDK](https://pypi.org/project/explainable-ai-sdk/) to generate most of the fields. \n",
    "\n",
    "The value for `input_baselines` tells the explanations service what the baseline input should be for your model. Here you're using the median for all of your input features. That means the baseline prediction for this model will be the trip duration your model predicts for the median of each feature in your dataset. \n",
    "\n",
    "Since this model accepts a single numpy array with all numerical feature, you can optionally pass an `index_feature_mapping` list to AI Explanations to make the API response easier to parse. When you provide a list of feature names via this parameter, the service will return a key / value mapping of each feature with its corresponding attribution value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UolAW3lcVTGl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 4), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\")\n",
      "Model input tensor name:  dense_input\n",
      "Model output tensor name:  dense_2/BiasAdd:0\n"
     ]
    }
   ],
   "source": [
    "# Print the names of your tensors\n",
    "print(model.input)\n",
    "print('Model input tensor name: ', model.input.name)\n",
    "print('Model output tensor name: ', model.output.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qpZiW9Cq6IY4"
   },
   "outputs": [],
   "source": [
    "from explainable_ai_sdk.metadata.tf.v2 import SavedModelMetadataBuilder\n",
    "builder = SavedModelMetadataBuilder(export_path)\n",
    "builder.set_numeric_metadata(\n",
    "    model.input.name.split(':')[0],\n",
    "    input_baselines=[train_data.median().values.tolist()],\n",
    "    index_feature_mapping=train_data.columns.tolist()\n",
    ")\n",
    "builder.save_metadata(export_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rT3iG5pDdrHi"
   },
   "source": [
    "Since this is a regression model (predicting a numerical value), the baseline prediction will be the same for every example you send to the model. If this were instead a classification model, each class would have a different baseline prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J6MKKy6Xb2MT"
   },
   "source": [
    "### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S2OaOycmb4o0"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "MODEL = 'natality' + datetime.datetime.now().strftime(\"%d%m%Y%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0bwCxEr5b8BP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://europe-west3-ml.googleapis.com/]\n",
      "Created ai platform model [projects/hack4ssb-team1/models/natality30092021114230].\n"
     ]
    }
   ],
   "source": [
    "# Create the model if it doesn't exist yet (you only need to run this once)\n",
    "! gcloud ai-platform models create $MODEL --enable-logging --region=$REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qp4qfnZib-zQ"
   },
   "source": [
    "### Create the model version \n",
    "\n",
    "Creating the version will take ~5-10 minutes. Note that your first deploy could take longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LQlcQFG_AB4o"
   },
   "outputs": [],
   "source": [
    "# Each time you create a version the name should be unique\n",
    "VERSION = 'v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3l5t2o1t7dal"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://europe-west3-ml.googleapis.com/]\n",
      "Explanations reflect patterns in your model, but don't necessarily reveal fundamental relationships about your data population. See https://cloud.google.com/ml-engine/docs/ai-explanations/limitations for more information.\n",
      "Creating version (this might take a few minutes)......done.                    \n"
     ]
    }
   ],
   "source": [
    "# Create the version with gcloud\n",
    "explain_method = 'integrated-gradients'\n",
    "! gcloud beta ai-platform versions create $VERSION --region=$REGION \\\n",
    "--model $MODEL \\\n",
    "--origin $export_path \\\n",
    "--runtime-version 2.5 \\\n",
    "--framework TENSORFLOW \\\n",
    "--python-version 3.7 \\\n",
    "--machine-type n1-standard-4 \\\n",
    "--explanation-method $explain_method \\\n",
    "--num-integral-steps 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eWkkRFhEMbFa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://europe-west3-ml.googleapis.com/]\n",
      "createTime: '2021-09-30T11:42:43Z'\n",
      "deploymentUri: gs://dev-xai/explanations/mymodel\n",
      "etag: csjbOCoLG9I=\n",
      "explanationConfig:\n",
      "  integratedGradientsAttribution:\n",
      "    numIntegralSteps: 25\n",
      "framework: TENSORFLOW\n",
      "isDefault: true\n",
      "machineType: n1-standard-4\n",
      "name: projects/hack4ssb-team1/models/natality30092021114230/versions/v1\n",
      "pythonVersion: '3.7'\n",
      "runtimeVersion: '2.5'\n",
      "state: READY\n"
     ]
    }
   ],
   "source": [
    "# Make sure the model deployed correctly. State should be `READY` in the following log\n",
    "! gcloud ai-platform versions describe $VERSION --model $MODEL --region=$REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "JzevJps9IOcU"
   },
   "source": [
    "## Get predictions and explanations\n",
    "\n",
    "Now that your model is deployed, you can use the AI Platform Prediction API to get feature attributions. You'll pass it a single test example here and see which features were most important in the model's prediction. Here you'll use the [Explainable AI SDK](https://pypi.org/project/explainable-ai-sdk/) to get your prediction and explanation. You can also use `gcloud`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CJ-2ErWJDvcg"
   },
   "source": [
    "### Format your explanation request\n",
    "\n",
    "To make your AI Explanations request, you need to create a JSON object with your test data for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D_PR2BcHD40-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense_input\n",
      "{'dense_input': [0.0, 34.0, 1.0, 40.0]}\n"
     ]
    }
   ],
   "source": [
    "# Format data for prediction to your model\n",
    "print(model.input.name)\n",
    "prediction_json = {model.input.name.split(':')[0]: test_data.iloc[0].values.tolist()}\n",
    "print(prediction_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kw7_f9QVD8Y_"
   },
   "source": [
    "### Send the explain request\n",
    "\n",
    "You can use the Explainable AI SDK to send explanation requests to your deployed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9K1yt7z69iXY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model 'natality30092021114230' version 'v1' in project 'hack4ssb-team1' on region europe-west3 from ai platform...\n",
      "Done predicting and explaining instances\n"
     ]
    }
   ],
   "source": [
    "print(f\"Load model '{MODEL}' version '{VERSION}' in project '{PROJECT_ID}' on region {REGION} from ai platform...\")\n",
    "remote_ig_model = explainable_ai_sdk.load_model_from_ai_platform(PROJECT_ID, MODEL, VERSION, region=REGION)\n",
    "instances = [prediction_json]\n",
    "predictions = remote_ig_model.predict(instances)\n",
    "ig_response = remote_ig_model.explain(instances)\n",
    "print(\"Done predicting and explaining instances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0nKR8RelNnkK"
   },
   "source": [
    "### Understanding the explanations response\n",
    "\n",
    "First, let's look at the trip duration your model predicted and compare it to the actual value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "825KoNgHR-tv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted weight: 7.15 pounds\n",
      "Actual weight: 9.25059651352 pounds\n"
     ]
    }
   ],
   "source": [
    "attr = ig_response[0].get_attribution()\n",
    "\n",
    "predicted = round(attr.example_score, 2)\n",
    "print('Predicted weight: ' + str(predicted) + ' pounds')\n",
    "print('Actual weight: ' + str(test_labels.iloc[0]) + ' pounds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QmObtmXIONDp"
   },
   "source": [
    "Next let's look at the feature attributions for this particular example. Positive attribution values mean a particular feature pushed your model prediction up by that amount, and vice versa for negative attribution values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6HKvAImeM_qi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Index 0\n",
      "Example Score: 7.1513\n",
      "Baseline Score: 7.1329\n",
      "Approximation Error: 0.0004\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ig_response[0].visualize_attributions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BZiM7kywQy6j"
   },
   "source": [
    "## Check your explanations and baselines\n",
    "\n",
    "To better make sense of the feature attributions you're getting, you should compare them with your model's baseline. In most cases, the sum of your attribution values + the baseline should be very close to your model's predicted value for each input. Also note that for regression models, the `baseline_score` returned from AI Explanations will be the same for each example sent to your model. For classification models, each class will have its own baseline.\n",
    "\n",
    "In this section you'll send 10 test examples to your model for prediction in order to compare the feature attributions with the baseline. Then you'll run each test example's attributions through two sanity checks in the `sanity_check_explanations` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CSf6psVDSDrN"
   },
   "outputs": [],
   "source": [
    "# Prepare 10 test examples to your model for prediction\n",
    "pred_batch = []\n",
    "for i in range(10):\n",
    "    pred_batch.append({model.input.name.split(':')[0]: test_data.iloc[i].values.tolist()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M-lqktTI9iXj"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Target URI https://europe-west3-ml.googleapis.com/v1/projects/hack4ssb-team1/models/natality30092021114230/versions/v1:explain returns HTTP 400 error.\nPlease check the raw error message: \n{\n  \"error\": {\n    \"code\": 400,\n    \"message\": \"{\\\"error\\\": \\\"Unable to explain the requested instance(s) because: Conversion failed. Key dense_3_input not in key map {'dense_input': 'dense_input'}.\\\"}\",\n    \"status\": \"INVALID_ARGUMENT\"\n  }\n}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19407/3922721259.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremote_ig_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/explainable_ai_sdk/model/ai_platform_model.py\u001b[0m in \u001b[0;36mexplain\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mrequest_body\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_credentials\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         timeout_ms)\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/explainable_ai_sdk/model/http_utils.py\u001b[0m in \u001b[0;36mmake_post_request_to_ai_platform\u001b[0;34m(uri, request_body, credentials, timeout_ms)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m   \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_handle_ai_platform_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/explainable_ai_sdk/model/http_utils.py\u001b[0m in \u001b[0;36m_handle_ai_platform_response\u001b[0;34m(uri, response)\u001b[0m\n\u001b[1;32m     74\u001b[0m   raise ValueError(('Target URI {} returns HTTP {} error.\\n'\n\u001b[1;32m     75\u001b[0m                     \u001b[0;34m'Please check the raw error message: \\n{}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                     ).format(uri, response.status_code, response.text))\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Target URI https://europe-west3-ml.googleapis.com/v1/projects/hack4ssb-team1/models/natality30092021114230/versions/v1:explain returns HTTP 400 error.\nPlease check the raw error message: \n{\n  \"error\": {\n    \"code\": 400,\n    \"message\": \"{\\\"error\\\": \\\"Unable to explain the requested instance(s) because: Conversion failed. Key dense_3_input not in key map {'dense_input': 'dense_input'}.\\\"}\",\n    \"status\": \"INVALID_ARGUMENT\"\n  }\n}\n"
     ]
    }
   ],
   "source": [
    "test_response = remote_ig_model.explain(pred_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vEevMCrMNjxm"
   },
   "source": [
    "In the function below you perform two sanity checks for models using Integrated Gradient (IG) explanations and one sanity check for models using Sampled Shapley."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B_WQXkE6RLe4"
   },
   "outputs": [],
   "source": [
    "def sanity_check_explanations(example, mean_tgt_value=None, variance_tgt_value=None):\n",
    "    passed_test = 0\n",
    "    total_test = 1\n",
    "    # `attributions` is a dict where keys are the feature names\n",
    "    # and values are the feature attributions for each feature\n",
    "    attr = example.get_attribution()\n",
    "    baseline_score = attr.baseline_score\n",
    "    # sum_with_baseline = np.sum(attribution_vals) + baseline_score\n",
    "    predicted_val = attr.example_score\n",
    "\n",
    "    # Sanity check 1\n",
    "    # The prediction at the input is equal to that at the baseline.\n",
    "    #  Please use a different baseline. Some suggestions are: random input, training\n",
    "    #  set mean.\n",
    "    if abs(predicted_val - baseline_score) <= 0.05:\n",
    "        print('Warning: example score and baseline score are too close.')\n",
    "        print('You might not get attributions.')\n",
    "    else:\n",
    "        passed_test += 1\n",
    "\n",
    "    # Sanity check 2 (only for models using Integrated Gradient explanations)\n",
    "    # Ideally, the sum of the integrated gradients must be equal to the difference\n",
    "    # in the prediction probability at the input and baseline. Any discrepency in\n",
    "    # these two values is due to the errors in approximating the integral.\n",
    "    if explain_method == 'integrated-gradients':\n",
    "        total_test += 1\n",
    "        want_integral = predicted_val - baseline_score\n",
    "        got_integral = sum(attr.post_processed_attributions.values())\n",
    "        if abs(want_integral - got_integral) / abs(want_integral) > 0.05:\n",
    "            print('Warning: Integral approximation error exceeds 5%.')\n",
    "            print('Please try increasing the number of integrated gradient steps.')\n",
    "        else:\n",
    "            passed_test += 1\n",
    "\n",
    "    print(passed_test, ' out of ', total_test, ' sanity checks passed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dkpK830AtRkJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: example score and baseline score are too close.\n",
      "You might not get attributions.\n",
      "1  out of  2  sanity checks passed.\n",
      "2  out of  2  sanity checks passed.\n",
      "Warning: example score and baseline score are too close.\n",
      "You might not get attributions.\n",
      "1  out of  2  sanity checks passed.\n",
      "2  out of  2  sanity checks passed.\n",
      "2  out of  2  sanity checks passed.\n",
      "2  out of  2  sanity checks passed.\n",
      "2  out of  2  sanity checks passed.\n",
      "2  out of  2  sanity checks passed.\n",
      "2  out of  2  sanity checks passed.\n",
      "Warning: example score and baseline score are too close.\n",
      "You might not get attributions.\n",
      "1  out of  2  sanity checks passed.\n"
     ]
    }
   ],
   "source": [
    "for response in test_response:\n",
    "    sanity_check_explanations(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C5ur65baOmnn"
   },
   "source": [
    "## Understanding AI Explanations with the What-If Tool\n",
    "\n",
    "In this section you'll use the [What-If Tool](https://pair-code.github.io/what-if-tool/) to better understand how your model is making predictions. See the cell below the What-if Tool for visualization ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OFO6s8ZsvKT_"
   },
   "source": [
    "The What-If-Tool expects data with keys for each feature name, but your model expects a flat list. The functions below convert data to the format required by the What-If Tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-yajpXi4Oyc3"
   },
   "outputs": [],
   "source": [
    "# This is the number of data points you'll send to the What-if Tool\n",
    "WHAT_IF_TOOL_SIZE = 500\n",
    "\n",
    "from witwidget.notebook.visualization import WitWidget, WitConfigBuilder\n",
    "\n",
    "\n",
    "def create_list(ex_dict):\n",
    "    new_list = []\n",
    "    for i in feature_names:\n",
    "        new_list.append(ex_dict[i])\n",
    "    return new_list\n",
    "\n",
    "\n",
    "def example_dict_to_input(example_dict):\n",
    "    return {'dense_input': create_list(example_dict)}\n",
    "\n",
    "\n",
    "from collections import OrderedDict\n",
    "wit_data = test_data.iloc[:WHAT_IF_TOOL_SIZE].copy()\n",
    "wit_data['duration'] = test_labels[:WHAT_IF_TOOL_SIZE]\n",
    "wit_data_dict = wit_data.to_dict(orient='records', into=OrderedDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3I2bEUe7Pr2Y"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "248b4df15968453daecccaad964d0ba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "WitWidget(config={'model_type': 'regression', 'label_vocab': [], 'uses_json_input': True, 'inference_address':â¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config_builder = WitConfigBuilder(\n",
    "    wit_data_dict\n",
    ").set_ai_platform_model(\n",
    "    PROJECT_ID,\n",
    "    MODEL,\n",
    "    VERSION,\n",
    "    adjust_example=example_dict_to_input\n",
    ").set_target_feature('duration').set_model_type('regression')\n",
    "\n",
    "WitWidget(config_builder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hWmNoC6FxOAt"
   },
   "source": [
    "### What-If Tool visualization ideas\n",
    "\n",
    "On the x-axis, you'll see the predicted trip duration for the test inputs you passed to the What-If Tool. Each circle represents one of your test examples. If you click on a circle, you'll be able to see the feature values for that example along with the attribution values for each feature. \n",
    "\n",
    "* You can edit individual feature values and re-run prediction directly within the What-If Tool. Try changing `distance`, click **Run inference** and see how that affects the model's prediction\n",
    "* You can sort features for an individual example by their attribution value, try changing the sort from the attributions dropdown\n",
    "* The What-If Tool also lets you create custom visualizations. You can do this by changing the values in the dropdown menus above the scatter plot visualization. For example, you can sort data points by inference error, or by their similarity to a single datapoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "x27DXeUGzb-M"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all GCP resources used in this project, you can [delete the GCP\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Alternatively, you can clean up individual resources by running the following\n",
    "commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": true,
    "editable": true,
    "id": "no210oWF68Uk"
   },
   "outputs": [],
   "source": [
    "# Delete model version resource\n",
    "# ! gcloud ai-platform versions delete $VERSION --quiet --model $MODEL --region $REGION\n",
    "\n",
    "# Delete model resource\n",
    "# ! gcloud ai-platform models delete $MODEL --quiet --region $REGION\n",
    "\n",
    "# Delete Cloud Storage objects that were created\n",
    "# ! gsutil -m rm -r gs://$BUCKET_NAME --region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "3F2g4OjbJ3gZ"
   },
   "source": [
    "If your Cloud Storage bucket doesn't contain any other objects and you would like to delete it, run `gsutil rm -r gs://$BUCKET_NAME`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "K0UXLWaBJnrY"
   },
   "source": [
    "## What's next?\n",
    "\n",
    "To learn more about AI Explanations or the What-if Tool, check out the resources here.\n",
    "\n",
    "* [AI Explanations documentation](cloud.google.com/ml-engine/docs/ai-explanations)\n",
    "* [Documentation for using the What-if Tool with Cloud AI Platform models ](https://cloud.google.com/ml-engine/docs/using-what-if-tool) \n",
    "* [What-If Tool documentation and demos](https://pair-code.github.io/what-if-tool/)\n",
    "* [Integrated gradients paper](https://arxiv.org/abs/1703.01365)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ai-explanations-tabular.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "common-cpu.m80",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m80"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
